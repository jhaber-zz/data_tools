{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general packages\n",
    "import imp, importlib # For working with modules\n",
    "import nltk # for natural language processing tools\n",
    "\n",
    "import pandas as pd # for working with dataframes\n",
    "#from pandas.core.groupby.groupby import PanelGroupBy # For debugging\n",
    "import numpy as np # for working with numbers\n",
    "import pickle # For working with .pkl files\n",
    "import sys # For terminal tricks\n",
    "import _pickle as cPickle # Optimized version of pickle\n",
    "import gc # For managing garbage collector\n",
    "import timeit # For counting time taken for a process\n",
    "import datetime # For working with dates & times\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "import ast\n",
    "from collections import Counter \n",
    "\n",
    "#do in terminal python3 -m pip install --upgrade pandas\n",
    "import statistics\n",
    "\n",
    "import string\n",
    "folder_prefix = '/home/jovyan/work/'\n",
    "\n",
    "# Import packages for cleaning, tokenizing, and stemming text\n",
    "import re # For parsing text\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words (it just cuts off the ends)\n",
    "from nltk.stem.porter import PorterStemmer # approximate but effective (and common) method of normalizing words: stems words by implementing a hierarchy of linguistic rules that transform or cut off word endings\n",
    "stem = PorterStemmer().stem # Makes stemming more accessible\n",
    "from nltk.corpus import stopwords # for eliminating stop words\n",
    "import gensim # For word embedding models\n",
    "from gensim.models.phrases import Phrases # Makes word2vec more robust: Looks not just at  To look for multi-word phrases within word2vec\n",
    "from gensim.test.utils import get_tmpfile \n",
    "\n",
    "\n",
    "# Import packages for multiprocessing\n",
    "import os # For navigation\n",
    "##numcpus = len(os.sched_getaffinity(0)) # Detect and assign number of available CPUs\n",
    "#from multiprocessing import Pool # key function for multiprocessing, to increase processing speed\n",
    "#pool = Pool(processes=numcpus) # Pre-load number of CPUs into pool function\n",
    "import Cython # For parallelizing word2vec\n",
    "#mpdo = False # Set to 'True' if using multiprocessing--faster for creating words by sentence file, but more complicated\n",
    "\n",
    "# FOR VISUALIZATIONS\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns # To make matplotlib prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import words\n",
    "english_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_make(vocab_path_old = \"\", extend_stopwords = False):\n",
    "    \"\"\"Create stopwords list. \n",
    "    If extend_stopwords is True, create larger stopword list by joining sklearn list to NLTK list.\"\"\"\n",
    "                                                     \n",
    "    stop_word_list = list(set(stopwords.words(\"english\"))) # list of english stopwords\n",
    "\n",
    "    # Add dates to stopwords\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "    for i in range(1, 2100):\n",
    "        stop_word_list.append(str(i))\n",
    "\n",
    "    # Add other common stopwords\n",
    "    stop_word_list.append('00') \n",
    "    stop_word_list.extend(['mr', 'mrs', 'sa', 'fax', 'email', 'phone', 'am', 'pm', 'org', 'com', \n",
    "                           'Menu', 'Contact Us', 'Facebook', 'Calendar', 'Lunch', 'Breakfast', \n",
    "                           'facebook', 'FAQs', 'FAQ', 'faq', 'faqs']) # web stopwords\n",
    "    stop_word_list.extend(['el', 'en', 'la', 'los', 'para', 'las', 'san']) # Spanish stopwords\n",
    "    stop_word_list.extend(['angeles', 'diego', 'harlem', 'bronx', 'austin', 'antonio']) # cities with many charter schools\n",
    "\n",
    "    # Add state names & abbreviations (both uppercase and lowercase) to stopwords\n",
    "    states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "              'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', \n",
    "              'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', \n",
    "              'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "              'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WI', 'WV', 'WY', \n",
    "              'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', \n",
    "              'Colorado', 'Connecticut', 'District of Columbia', 'Delaware', 'Florida', \n",
    "              'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', \n",
    "              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', \n",
    "              'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "              'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "              'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', \n",
    "              'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', \n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', \n",
    "              'Vermont', 'Virginia', 'Washington', 'Wisconsin', 'West Virginia', 'Wyoming' \n",
    "              'carolina', 'columbia', 'dakota', 'hampshire', 'mexico', 'rhode', 'york']\n",
    "    for state in states:\n",
    "        stop_word_list.append(state)\n",
    "    for state in [state.lower() for state in states]:\n",
    "        stop_word_list.append(state)\n",
    "        \n",
    "    # Add even more stop words:\n",
    "    if extend_stopwords == True:\n",
    "        stop_word_list = text.ENGLISH_STOP_WORDS.union(stop_word_list)\n",
    "        \n",
    "    # If path to old vocab not specified, skip last step and return stop word list thus far\n",
    "    if vocab_path_old == \"\":\n",
    "        return stop_word_list\n",
    "\n",
    "    # Add to stopwords useless and hard-to-formalize words/chars from first chunk of previous model vocab (e.g., a3d0, \\fs19)\n",
    "    # First create whitelist of useful terms probably in that list, explicitly exclude from junk words list both these and words with underscores (common phrases)\n",
    "    whitelist = [\"Pre-K\", \"pre-k\", \"pre-K\", \"preK\", \"prek\", \n",
    "                 \"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\", \"12th\", \n",
    "                 \"1st-grade\", \"2nd-grade\", \"3rd-grade\", \"4th-grade\", \"5th-grade\", \"6th-grade\", \n",
    "                 \"7th-grade\", \"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\", \"12th-grade\", \n",
    "                 \"1st-grader\", \"2nd-grader\", \"3rd-grader\", \"4th-grader\", \"5th-grader\", \"6th-grader\", \n",
    "                 \"7th-grader\", \"8th-grader\", \"9th-grader\", \"10th-grader\", \"11th-grader\", \"12th-grader\", \n",
    "                 \"1stgrade\", \"2ndgrade\", \"3rdgrade\", \"4thgrade\", \"5thgrade\", \"6thgrade\", \n",
    "                 \"7thgrade\", \"8thgrade\", \"9thgrade\", \"10thgrade\", \"11thgrade\", \"12thgrade\", \n",
    "                 \"1stgrader\", \"2ndgrader\", \"3rdgrader\", \"4thgrader\", \"5thgrader\", \"6thgrader\", \n",
    "                 \"7thgrader\", \"8thgrader\", \"9thgrader\", \"10thgrader\", \"11thgrader\", \"12thgrader\"]\n",
    "    with open(vocab_path_old) as f: # Load vocab from previous model\n",
    "        junk_words = f.read().splitlines() \n",
    "    junk_words = [word for word in junk_words[:8511] if ((not \"_\" in word) \n",
    "                                                         and (not any(term in word for term in whitelist)))]\n",
    "    stop_word_list.extend(junk_words)\n",
    "                                                     \n",
    "    return stop_word_list\n",
    "                                                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctstr_make():\n",
    "    \"\"\"Creates punctuations list\"\"\"\n",
    "                    \n",
    "    punctuations = list(string.punctuation) # assign list of common punctuation symbols\n",
    "    #addpuncts = ['*','•','©','–','`','’','“','”','»','.','×','|','_','§','…','⎫'] # a few more punctuations also common in web text\n",
    "    #punctuations += addpuncts # Expand punctuations list\n",
    "    #punctuations = list(set(punctuations)) # Remove duplicates\n",
    "    punctuations.remove('-') # Don't remove hyphens - dashes at beginning and end of words are handled separately)\n",
    "    punctuations.remove(\"'\") # Don't remove possessive apostrophes - those at beginning and end of words are handled separately\n",
    "    punctstr = \"\".join([char for char in punctuations]) # Turn into string for regex later\n",
    "\n",
    "    return punctstr\n",
    "                                                     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    \n",
    "def unicode_make():\n",
    "    \"\"\"Create list of unicode chars\"\"\"\n",
    "                    \n",
    "    unicode_list  = []\n",
    "    for i in range(1000,3000):\n",
    "        unicode_list.append(chr(i))\n",
    "    unicode_list.append(\"_cid:10\") # Common in webtext junk\n",
    "                                                     \n",
    "    return unicode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(tokenized_corpus, max_percentage):\n",
    "    \"\"\"Discover most common words in corpus up to max_percentage.\n",
    "    \n",
    "    Args:\n",
    "        Corpus tokenized by words,\n",
    "        Highest allowable frequency of documents in which a token may appear (e.g., 1-5%)\n",
    "        \n",
    "    Returns:\n",
    "        List of most frequent words in corpus\"\"\"\n",
    "    \n",
    "    # Code goes here\n",
    "    # Probably using nltk.CountVectorizer\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create useful lists using above functions:\n",
    "stop_words_list = stopwords_make()\n",
    "punctstr = punctstr_make()\n",
    "unicode_list = unicode_make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, english_words, remove_stopwords = True, most_common_words = [], stemming=False):\n",
    "    \"\"\"Removes numbers, emails, URLs, unicode characters, hex characters, and punctuation from a sentence \n",
    "    separated by whitespaces. Returns a tokenized, cleaned list of words from the sentence.\n",
    "    \n",
    "    Args: \n",
    "        Sentence, i.e. string that possibly includes spaces and punctuation\n",
    "    Returns: \n",
    "        Cleaned & tokenized sentence, i.e. a list of cleaned, lower-case, one-word strings\"\"\"\n",
    "    \n",
    "    global stop_words_list, punctstr, unicode_list, stem\n",
    "    \n",
    "    # Replace unicode spaces, tabs, and underscores with spaces, and remove whitespaces from start/end of sentence:\n",
    "    sentence = sentence.replace(u\"\\xa0\", u\" \").replace(u\"\\\\t\", u\" \").replace(u\"_\", u\" \").strip(\" \")\n",
    "    \n",
    "    # Remove hex characters (e.g., \\xa0\\, \\x80):\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]', r'', sentence) #replace anything that starts with a hex character \n",
    "\n",
    "    # Replace \\\\x, \\\\u, \\\\b, or anything that ends with \\u2605\n",
    "    sentence = re.sub(r\"\\\\x.*|\\\\u.*|\\\\b.*|\\u2605$\", \"\", sentence)\n",
    "        \n",
    "    # Remove all elements that appear in unicode_list (looks like r'u1000|u10001|'):\n",
    "    sentence = re.sub(r'|'.join(map(re.escape, unicode_list)), '', sentence)\n",
    "    \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence) # Remove numbers\n",
    "    \n",
    "    sent_list = [] # Initialize empty list to hold tokenized sentence (words added one at a time)\n",
    "    \n",
    "    for word in sentence.split(): # Split by spaces and iterate over words\n",
    "        \n",
    "        word = word.strip() # Remove leading and trailing spaces\n",
    "        \n",
    "        # Filter out emails and URLs:\n",
    "        if (\"@\" in word or word.startswith(('http', 'https', 'www', '//', '\\\\', 'x_', 'x/', 'srcimage')) or word.endswith(('.com', '.net', '.gov', '.org', '.jpg', '.pdf', 'png', 'jpeg', 'php'))):\n",
    "            continue\n",
    "            \n",
    "        # Remove punctuation (only after URLs removed):\n",
    "        word = re.sub(r\"[\"+punctstr+\"]+\", r'', word).strip(\"'\").strip(\"-\") # Remove punctuations, and remove dashes and apostrophes only from start/end of words\n",
    "        print(word)\n",
    "        if remove_stopwords and word in stop_words_list: # Filter out stop words\n",
    "            continue\n",
    "\n",
    "        # TO DO: Pass in most_common_words to function; write function to find the top 1-5% most frequent words, which we will exclude\n",
    "        # Remove most common words:\n",
    "        if word in most_common_words:\n",
    "            continue\n",
    "        if word not in english_words:\n",
    "            continue\n",
    "        # Stem word (if applicable):\n",
    "        if stemming:\n",
    "            word = ps.stem(word)\n",
    "        \n",
    "        sent_list.append(word.lower()) # Add lower-cased word to list (after passing checks)\n",
    "\n",
    "    return sent_list # Return clean, tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hoa\n",
      "como\n",
      "estass\n",
      "Hello\n",
      "hpw\n",
      "are\n",
      "you\n",
      "The\n",
      "district\n",
      "is\n",
      "safer\n",
      "here\n",
      "Alameda\n",
      "county\n",
      "namaste\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'district', 'county']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \" hoa, como estass, Hello hpw are you. The district is safer here. Alameda county namaste ?\"\n",
    "clean_sentence(s, english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
