{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general packages\n",
    "import imp, importlib # For working with modules\n",
    "import nltk # for natural language processing tools\n",
    "\n",
    "import pandas as pd # for working with dataframes\n",
    "#from pandas.core.groupby.groupby import PanelGroupBy # For debugging\n",
    "import numpy as np # for working with numbers\n",
    "import pickle # For working with .pkl files\n",
    "import sys # For terminal tricks\n",
    "import _pickle as cPickle # Optimized version of pickle\n",
    "import gc # For managing garbage collector\n",
    "import timeit # For counting time taken for a process\n",
    "import datetime # For working with dates & times\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "import ast\n",
    "from collections import Counter \n",
    "\n",
    "#do in terminal python3 -m pip install --upgrade pandas\n",
    "import statistics\n",
    "\n",
    "import string\n",
    "folder_prefix = '/home/jovyan/work/'\n",
    "\n",
    "# Import packages for cleaning, tokenizing, and stemming text\n",
    "import re # For parsing text\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words (it just cuts off the ends)\n",
    "from nltk.stem.porter import PorterStemmer # approximate but effective (and common) method of normalizing words: stems words by implementing a hierarchy of linguistic rules that transform or cut off word endings\n",
    "stem = PorterStemmer().stem # Makes stemming more accessible\n",
    "from nltk.corpus import stopwords # for eliminating stop words\n",
    "import gensim # For word embedding models\n",
    "from gensim.models.phrases import Phrases # Makes word2vec more robust: Looks not just at  To look for multi-word phrases within word2vec\n",
    "from gensim.test.utils import get_tmpfile \n",
    "\n",
    "\n",
    "# Import packages for multiprocessing\n",
    "import os # For navigation\n",
    "##numcpus = len(os.sched_getaffinity(0)) # Detect and assign number of available CPUs\n",
    "#from multiprocessing import Pool # key function for multiprocessing, to increase processing speed\n",
    "#pool = Pool(processes=numcpus) # Pre-load number of CPUs into pool function\n",
    "import Cython # For parallelizing word2vec\n",
    "#mpdo = False # Set to 'True' if using multiprocessing--faster for creating words by sentence file, but more complicated\n",
    "\n",
    "# FOR VISUALIZATIONS\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns # To make matplotlib prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> stopwords\n",
      "Command 'stopwords' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "english_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_make(vocab_path_old = \"\", extend_stopwords = False):\n",
    "    \"\"\"Create stopwords list. \n",
    "    If extend_stopwords is True, create larger stopword list by joining sklearn list to NLTK list.\"\"\"\n",
    "                                                     \n",
    "    stop_word_list = list(set(stopwords.words(\"english\"))) # list of english stopwords\n",
    "\n",
    "    # Add dates to stopwords\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "    for i in range(1, 2100):\n",
    "        stop_word_list.append(str(i))\n",
    "\n",
    "    # Add other common stopwords\n",
    "    stop_word_list.append('00') \n",
    "    stop_word_list.extend(['mr', 'mrs', 'sa', 'fax', 'email', 'phone', 'am', 'pm', 'org', 'com', \n",
    "                           'Menu', 'Contact Us', 'Facebook', 'Calendar', 'Lunch', 'Breakfast', \n",
    "                           'facebook', 'FAQs', 'FAQ', 'faq', 'faqs']) # web stopwords\n",
    "    stop_word_list.extend(['el', 'en', 'la', 'los', 'para', 'las', 'san']) # Spanish stopwords\n",
    "    stop_word_list.extend(['angeles', 'diego', 'harlem', 'bronx', 'austin', 'antonio']) # cities with many charter schools\n",
    "\n",
    "    # Add state names & abbreviations (both uppercase and lowercase) to stopwords\n",
    "    states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "              'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', \n",
    "              'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', \n",
    "              'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "              'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WI', 'WV', 'WY', \n",
    "              'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', \n",
    "              'Colorado', 'Connecticut', 'District of Columbia', 'Delaware', 'Florida', \n",
    "              'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', \n",
    "              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', \n",
    "              'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "              'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "              'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', \n",
    "              'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', \n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', \n",
    "              'Vermont', 'Virginia', 'Washington', 'Wisconsin', 'West Virginia', 'Wyoming' \n",
    "              'carolina', 'columbia', 'dakota', 'hampshire', 'mexico', 'rhode', 'york']\n",
    "    for state in states:\n",
    "        stop_word_list.append(state)\n",
    "    for state in [state.lower() for state in states]:\n",
    "        stop_word_list.append(state)\n",
    "        \n",
    "    # Add even more stop words:\n",
    "    if extend_stopwords == True:\n",
    "        stop_word_list = text.ENGLISH_STOP_WORDS.union(stop_word_list)\n",
    "        \n",
    "    # If path to old vocab not specified, skip last step and return stop word list thus far\n",
    "    if vocab_path_old == \"\":\n",
    "        return stop_word_list\n",
    "\n",
    "    # Add to stopwords useless and hard-to-formalize words/chars from first chunk of previous model vocab (e.g., a3d0, \\fs19)\n",
    "    # First create whitelist of useful terms probably in that list, explicitly exclude from junk words list both these and words with underscores (common phrases)\n",
    "    whitelist = [\"Pre-K\", \"pre-k\", \"pre-K\", \"preK\", \"prek\", \n",
    "                 \"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\", \"12th\", \n",
    "                 \"1st-grade\", \"2nd-grade\", \"3rd-grade\", \"4th-grade\", \"5th-grade\", \"6th-grade\", \n",
    "                 \"7th-grade\", \"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\", \"12th-grade\", \n",
    "                 \"1st-grader\", \"2nd-grader\", \"3rd-grader\", \"4th-grader\", \"5th-grader\", \"6th-grader\", \n",
    "                 \"7th-grader\", \"8th-grader\", \"9th-grader\", \"10th-grader\", \"11th-grader\", \"12th-grader\", \n",
    "                 \"1stgrade\", \"2ndgrade\", \"3rdgrade\", \"4thgrade\", \"5thgrade\", \"6thgrade\", \n",
    "                 \"7thgrade\", \"8thgrade\", \"9thgrade\", \"10thgrade\", \"11thgrade\", \"12thgrade\", \n",
    "                 \"1stgrader\", \"2ndgrader\", \"3rdgrader\", \"4thgrader\", \"5thgrader\", \"6thgrader\", \n",
    "                 \"7thgrader\", \"8thgrader\", \"9thgrader\", \"10thgrader\", \"11thgrader\", \"12thgrader\"]\n",
    "    with open(vocab_path_old) as f: # Load vocab from previous model\n",
    "        junk_words = f.read().splitlines() \n",
    "    junk_words = [word for word in junk_words[:8511] if ((not \"_\" in word) \n",
    "                                                         and (not any(term in word for term in whitelist)))]\n",
    "    stop_word_list.extend(junk_words)\n",
    "                                                     \n",
    "    return stop_word_list\n",
    "                                                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctstr_make():\n",
    "    \"\"\"Creates punctuations list\"\"\"\n",
    "                    \n",
    "    punctuations = list(string.punctuation) # assign list of common punctuation symbols\n",
    "    #addpuncts = ['*','•','©','–','`','’','“','”','»','.','×','|','_','§','…','⎫'] # a few more punctuations also common in web text\n",
    "    #punctuations += addpuncts # Expand punctuations list\n",
    "    #punctuations = list(set(punctuations)) # Remove duplicates\n",
    "    punctuations.remove('-') # Don't remove hyphens - dashes at beginning and end of words are handled separately)\n",
    "    punctuations.remove(\"'\") # Don't remove possessive apostrophes - those at beginning and end of words are handled separately\n",
    "    punctstr = \"\".join([char for char in punctuations]) # Turn into string for regex later\n",
    "\n",
    "    return punctstr\n",
    "                                                     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    \n",
    "def unicode_make():\n",
    "    \"\"\"Create list of unicode chars\"\"\"\n",
    "                    \n",
    "    unicode_list  = []\n",
    "    for i in range(1000,3000):\n",
    "        unicode_list.append(chr(i))\n",
    "    unicode_list.append(\"_cid:10\") # Common in webtext junk\n",
    "                                                     \n",
    "    return unicode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(tokenized_corpus, max_percentage):\n",
    "    \"\"\"Discover most common words in corpus up to max_percentage.\n",
    "    \n",
    "    Args:\n",
    "        Corpus tokenized by words,\n",
    "        Highest allowable frequency of documents in which a token may appear (e.g., 1-5%)\n",
    "        \n",
    "    Returns:\n",
    "        List of most frequent words in corpus\"\"\"\n",
    "    \n",
    "    # Code goes here\n",
    "    # Probably using nltk.CountVectorizer\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4aa098369631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create useful lists using above functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop_words_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpunctstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunctstr_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0municode_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1a421913ff0f>\u001b[0m in \u001b[0;36mstopwords_make\u001b[0;34m(vocab_path_old, extend_stopwords)\u001b[0m\n\u001b[1;32m      3\u001b[0m     If extend_stopwords is True, create larger stopword list by joining sklearn list to NLTK list.\"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstop_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list of english stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Add dates to stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Create useful lists using above functions:\n",
    "stop_words_list = stopwords_make()\n",
    "punctstr = punctstr_make()\n",
    "unicode_list = unicode_make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, english_words, remove_stopwords = True, most_common_words = [], stemming=False):\n",
    "    \"\"\"Removes numbers, emails, URLs, unicode characters, hex characters, and punctuation from a sentence \n",
    "    separated by whitespaces. Returns a tokenized, cleaned list of words from the sentence.\n",
    "    \n",
    "    Args: \n",
    "        Sentence, i.e. string that possibly includes spaces and punctuation\n",
    "    Returns: \n",
    "        Cleaned & tokenized sentence, i.e. a list of cleaned, lower-case, one-word strings\"\"\"\n",
    "    \n",
    "    global stop_words_list, punctstr, unicode_list, stem\n",
    "    \n",
    "    # Replace unicode spaces, tabs, and underscores with spaces, and remove whitespaces from start/end of sentence:\n",
    "    sentence = sentence.replace(u\"\\xa0\", u\" \").replace(u\"\\\\t\", u\" \").replace(u\"_\", u\" \").strip(\" \")\n",
    "    \n",
    "    # Remove hex characters (e.g., \\xa0\\, \\x80):\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]', r'', sentence) #replace anything that starts with a hex character \n",
    "\n",
    "    # Replace \\\\x, \\\\u, \\\\b, or anything that ends with \\u2605\n",
    "    sentence = re.sub(r\"\\\\x.*|\\\\u.*|\\\\b.*|\\u2605$\", \"\", sentence)\n",
    "        \n",
    "    # Remove all elements that appear in unicode_list (looks like r'u1000|u10001|'):\n",
    "    sentence = re.sub(r'|'.join(map(re.escape, unicode_list)), '', sentence)\n",
    "    \n",
    "    sentence = re.sub(\"\\d+\", \"\", sentence) # Remove numbers\n",
    "    \n",
    "    sent_list = [] # Initialize empty list to hold tokenized sentence (words added one at a time)\n",
    "    \n",
    "    for word in sentence.split(): # Split by spaces and iterate over words\n",
    "        \n",
    "        word = word.strip() # Remove leading and trailing spaces\n",
    "        \n",
    "        # Filter out emails and URLs:\n",
    "        if (\"@\" in word or word.startswith(('http', 'https', 'www', '//', '\\\\', 'x_', 'x/', 'srcimage')) or word.endswith(('.com', '.net', '.gov', '.org', '.jpg', '.pdf', 'png', 'jpeg', 'php'))):\n",
    "            continue\n",
    "            \n",
    "        # Remove punctuation (only after URLs removed):\n",
    "        word = re.sub(r\"[\"+punctstr+\"]+\", r'', word).strip(\"'\").strip(\"-\") # Remove punctuations, and remove dashes and apostrophes only from start/end of words\n",
    "        \n",
    "        if remove_stopwords and word in stop_words_list: # Filter out stop words\n",
    "            continue\n",
    "        if word not in english_words:\n",
    "            continue\n",
    "        # TO DO: Pass in most_common_words to function; write function to find the top 1-5% most frequent words, which we will exclude\n",
    "        # Remove most common words:\n",
    "        if word in most_common_words:\n",
    "            continue\n",
    "        \n",
    "        # Stem word (if applicable):\n",
    "        if stemming:\n",
    "            word = ps.stem(word)\n",
    "        \n",
    "        sent_list.append(word.lower()) # Add lower-cased word to list (after passing checks)\n",
    "\n",
    "    return sent_list # Return clean, tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c2b52e08c711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" hoa, como estass, Hello hpw are you?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-53c08aa91658>\u001b[0m in \u001b[0;36mclean_sentence\u001b[0;34m(sentence, english_words, remove_stopwords, most_common_words, stemming)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Remove all elements that appear in unicode_list (looks like r'u1000|u10001|'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\d+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Remove numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode_list' is not defined"
     ]
    }
   ],
   "source": [
    "s = \" hoa, como estass, Hello hpw are you?\"\n",
    "clean_sentence(sentence= s, english_words = english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
